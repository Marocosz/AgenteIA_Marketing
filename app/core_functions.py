import streamlit as st
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from pathlib import Path
import json
import pandas as pd
import csv
import os
from docling.document_converter import DocumentConverter

# Imports dos nossos novos arquivos
from config import *
from prompts import *


# --- FUN√á√ïES GERAIS ---

@st.cache_resource  # Decorator de ativar o cache do resultado da func
def load_llm():
    """Carrega o modelo de linguagem uma √∫nica vez e o guarda em cache."""
    llm = ChatGroq(
        model=MODEL_ID,
        temperature=TEMPERATURE,
        max_tokens=None,
        timeout=None,
        max_retries=2,
    )
    return llm


# --- FUN√á√ïES DO GERADOR DE CONTE√öDO ---

@st.cache_resource  # Decorator de ativar o cache do resultado da func
# OBS: O '_' no par√¢metro √© uma conven√ß√£o para o sistema de cache do Streamlit
def get_content_generation_chain(_llm):
    """
    Cria a cadeia de prompt e a armazena em cache para reutiliza√ß√£o.
    """
    # √â uma fun√ß√£o do LangChain que constr√≥i um template a partir de uma lista de mensagens
    template = ChatPromptTemplate.from_messages([
        # ("system", ...): Define a persona e a instru√ß√£o principal da IA para esta tarefa.
        ("system", CONTENT_GENERATOR_SYSTEM_PROMPT),
        ("human", "{prompt}"),  # Aqui seria oq o usu√°rio escrever ou selecionar
    ])
    # Corrente de acontecimentos do langchain
    # O parser √© pra fazer a resposta da llm virar us√°vel para gente aqui no c√≥digo
    chain = template | _llm | StrOutputParser()
    return chain


# --- FUN√á√ïES DO ATENDIMENTO SAFE BANK (RAG) ---

@st.cache_resource  # Decorator de ativar o cache do resultado da func (para rodar 1 vez s√≥ visto q ela √© pesada)
def config_retriever(_folder_path):

    docs_path = Path(_folder_path)  # Carrega o local do documento
    pdf_files = [f for f in docs_path.glob("*.pdf")]  # Pegam todos arquivos .pdf que est√£o na pasta

    if not pdf_files:
        st.error(f"Nenhum arquivo PDF encontrado no caminho: {_folder_path}")
        st.stop()  # Interrompe o script

    # Aqui extrai todo conte√∫do dos pdfs e guarda numa lista de listas das p√°ginas dos pdfs
    docs_list = [PyMuPDFLoader(str(pdf)).load() for pdf in pdf_files]

    # Aqui n√≥s pegamos e deixamos o conteudo todo em uma lista s√≥
    docs = [item for sublist in docs_list for item in sublist]

    # Aqui n√≥s criamos a fun√ß√£o de separa√ß√£o de texto de acordo com a qt de caracteres queremos e quanto cada separa√ß√£o compartilhar√° (chunk_overlap)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)

    chunks = text_splitter.split_documents(docs)  # Guardamos em uma lista os "chunks" dos textos dos pdfs

    # Aqui acontece o embedding que √© transformar cada chunk de texto em uma lista de n√∫meros (um vetor)
    # que representa o seu significado, de acordo com o modelo definido
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)

    # Aqui √© onde de fato a fun√ß√£o de Embeddings acontece e √© salvo num "banco de dados de vetores" (FAISS)
    # para permitir buscas por similaridade de forma instantanea
    vectorstore = FAISS.from_documents(chunks, embedding=embeddings)

    # Aqui n√≥s recuperamos (retriever) as informa√ß√µes dos documentos que est√£o em vetor no FAISS de acordo com oq precisamos de fato
    # E o "fetch_k" define quantos documentos o sistema busca inicialmente, ANTES de aplicar o algoritmo de diversidade (mmr) para
    # selecionar os 'k' melhores. Um 'fetch_k' maior d√° mais op√ß√µes para o algoritmo escolher, podendo resultar em respostas melhores.
    retriever = vectorstore.as_retriever(
        search_type=SEARCH_TYPE, search_kwargs={'k': SEARCH_K, 'fetch_k': FETCH_K})
    return retriever


@st.cache_resource   # Decorator de ativar o cache do resultado da func (para rodar 1 vez s√≥ visto q ela √© pesada)
def config_rag_chain(_llm, _retriever):

    # √â uma fun√ß√£o do LangChain que constr√≥i um template a partir de uma lista de mensagens
    # Esse √© o prompt que pegar√° o historico, a pipeline
    # Esse √© o prompt que vai refazer a pergunta do usu√°rio, para melhorar a pergunta dando os contextos
    context_q_prompt = ChatPromptTemplate.from_messages(
        [("system", CONTEXT_Q_SYSTEM_PROMPT),
         # Aqui √© permitido receber uma lista de textos, no caso o historico do chat
         MessagesPlaceholder("chat_history"),
         ("human", "{input}")])

    # Aqui √© a subchain que vai ser "um pesquisador ciente do hist√≥rico".
    # Seu output √© os trechos dos pdfs uteis
    history_aware_retriever = create_history_aware_retriever(
        llm=_llm, retriever=_retriever, prompt=context_q_prompt)

    # Aqui √© onde ser√° o prompt final que a IA responder√° de fato
    qa_prompt = ChatPromptTemplate.from_messages([("system", QA_SYSTEM_PROMPT),
                                                 MessagesPlaceholder(
                                                     "chat_history"),
                                                 # Pergunta original do usu√°rio
                                                 ("human", "{input}")])

    # Aqui √© a segunda subchain que junta o resultado do retriever e coloca o context dentro do qa_prompt
    qa_chain = create_stuff_documents_chain(_llm, qa_prompt)

    # E aqui √© a jun√ß√£o das duas chain que dar√° de fato a resposta final da IA
    rag_chain = create_retrieval_chain(history_aware_retriever, qa_chain)
    return rag_chain


# --- FUN√á√ïES DO ANALISADOR DE CURR√çCULOS ---

@st.cache_resource
def get_cv_analysis_chain(_llm):
    """Cria e armazena em cache a chain para an√°lise de curr√≠culos."""
    chain = CV_PROMPT_TEMPLATE | _llm | StrOutputParser() # Adicionado o StrOutputParser
    return chain

def parse_doc(file_path):
    """Converte um documento (PDF, DOCX) para Markdown."""
    converter = DocumentConverter()
    result = converter.convert(file_path)
    return result.document.export_to_markdown()

def parse_res_llm(response_text: str, required_fields: list) -> dict:
    """Extrai um objeto JSON da resposta de texto do LLM."""
    try:
        if "</think>" in response_text:
            response_text = response_text.split("</think>")[-1].strip()
        
        start_idx = response_text.find('{')
        end_idx = response_text.rfind('}') + 1
        
        if start_idx == -1 or end_idx == 0:
            st.error("Nenhum JSON v√°lido encontrado na resposta do modelo.")
            return None # Retorna None em caso de erro

        json_str = response_text[start_idx:end_idx]
        info_cv = json.loads(json_str)

        for field in required_fields:
            if field not in info_cv:
                info_cv[field] = "N/A" # Preenche campos ausentes
        return info_cv
    except json.JSONDecodeError:
        st.error("Erro ao decodificar o JSON da resposta do modelo.")
        return None # Retorna None em caso de erro

def save_json_cv(new_data, path_json, key_name="name"):
    """Salva os dados de um novo curr√≠culo em um arquivo JSON, evitando duplicatas."""
    if os.path.exists(path_json):
        with open(path_json, "r", encoding="utf-8") as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError:
                data = [] # Se o arquivo estiver corrompido/vazio, come√ßa uma nova lista
    else:
        data = []

    if not isinstance(data, list):
        data = [] # Garante que estamos trabalhando com uma lista

    candidates = [entry.get(key_name) for entry in data]
    if new_data.get(key_name) in candidates:
        st.warning(f"Curr√≠culo '{new_data.get(key_name)}' j√° registrado. Ignorando.")
        return

    data.append(new_data)
    with open(path_json, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)

def show_cv_result(result: dict):
    """Formata os dados extra√≠dos do CV em uma string Markdown para exibi√ß√£o."""
    md = f"### üìÑ An√°lise e Resumo do Curr√≠culo\n"
    
    # Usar .get() √© uma pr√°tica mais segura para evitar erros se uma chave n√£o existir
    if result.get("name"): 
        md += f"- **Nome:** {result.get('name', 'N/A')}\n"
    if result.get("area"): 
        md += f"- **√Årea de Atua√ß√£o:** {result.get('area', 'N/A')}\n"
    if result.get("summary"): 
        md += f"- **Resumo do Perfil:** {result.get('summary', 'N/A')}\n"
    if result.get("skills"): 
        md += f"- **Compet√™ncias:** {', '.join(result.get('skills', []))}\n"
    
    # --- AQUI EST√ÉO OS CAMPOS QUE FALTAVAM ---
    if result.get("interview_questions"):
        md += f"- **Perguntas sugeridas:**\n"
        md += "\n".join([f"  - {q}" for q in result.get("interview_questions", [])]) + "\n"
    if result.get("strengths"):
        md += f"- **Pontos fortes (ou Alinhamentos):**\n"
        md += "\n".join([f"  - {s}" for s in result.get("strengths", [])]) + "\n"
    if result.get("areas_for_development"):
        md += f"- **Pontos a desenvolver (ou Desalinhamentos):**\n"
        md += "\n".join([f"  - {a}" for a in result.get("areas_for_development", [])]) + "\n"
    if result.get("important_considerations"):
        md += f"- **Pontos de aten√ß√£o:**\n"
        md += "\n".join([f"  - {i}" for i in result.get("important_considerations", [])]) + "\n"
    if result.get("final_recommendations"):
        md += f"- **Conclus√£o e recomenda√ß√µes:** {result.get('final_recommendations', 'N/A')}\n"
        
    return md

def save_job_to_csv(data, filename):
    """Salva a descri√ß√£o da vaga em um CSV."""
    headers = ['title', 'description', 'details']
    file_exists = os.path.exists(filename)
    with open(filename, 'a', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=headers, delimiter=';')
        if not file_exists:
            writer.writeheader()
        writer.writerow(data)

def load_job(csv_path):
    """Carrega a √∫ltima vaga salva do CSV."""
    try:
        df = pd.read_csv(csv_path, sep=';', encoding='utf-8')
        if df.empty:
            return "Nenhuma vaga registrada."
        job = df.iloc[-1]
        prompt_text = f"**Vaga para {job['title']}**\n\n**Descri√ß√£o:**\n{job['description']}\n\n**Detalhes:**\n{job['details']}"
        return prompt_text.strip()
    except FileNotFoundError:
        return "Arquivo de vagas n√£o encontrado."

def display_json_table(path_json):
    """Carrega o JSON e o converte em um DataFrame do Pandas para exibi√ß√£o."""
    try:
        with open(path_json, "r", encoding="utf-8") as f:
            data = json.load(f)
        return pd.DataFrame(data)
    except (FileNotFoundError, json.JSONDecodeError):
        return pd.DataFrame() # Retorna um DataFrame vazio em caso de erro